---
title: Hands-on training session 3
subtitle: Hui-Walter models with more than two diagnostic tests
date: "`r Sys.Date()`"
author:
  - Matt Denwood
  - Giles Innocent
  - Sonja Hartnack
theme: metropolis
aspectratio: 43
colortheme: seahorse
header-includes: 
  - \input{preamble}
params:
  presentation: TRUE
output:
  beamer_presentation:
      pandoc_args: ["-t", "beamer"]
      slide_level: 2
  html_document: default
---

```{r rendering, eval=FALSE, include=FALSE}
# To render this as PDF (beamer) slides run:
rmarkdown::render('Adv3_MultipleTests.Rmd', 'beamer_presentation', params=list(presentation=TRUE))
# And for html:
rmarkdown::render('Adv3_MultipleTests.Rmd', 'html_document', params=list(presentation=FALSE))
```

```{r setup, include=FALSE}
# Reduce the width of R code output for PDF only:
if(params$presentation) options(width=60)
knitr::opts_chunk$set(echo = TRUE)

library('runjags')
runjags.options(silent.jags=TRUE)

set.seed(2020-02-18)
```


# Introduction

## Overview

Date/time:

  - 20th February 2020
  - 14.00 - 15.30

Teachers:

  - Matt Denwood (presenter)
  - Giles Innocent
  - Sonja Hartnack


## Recap

- JAGS / runjags is the easy way to work with complex models
  * But we *still have to* check convergence and effective sample size!

- Estimating sensitivity and specificity is like pulling a rabbit out of a hat

  * Multiple populations helps **a lot**
  * Strong priors for one of the tests helps even more!

. . .

- But what if the tests are not independent of each other?

# Session 3a:  Hui-Walter models for multiple conditionally independent tests

## What exactly is our latent class?

- What do we mean by "conditionally independent?"

. . .

- Example:  we have three antibody tests
  
  * The latent status is actually 'producing antibodies' not 'diseased'
  
. . .

- Example:  antibody vs egg count tests for liver fluke

  * Does the latent state include migrating juvenile fluke?

. . .

- We're actually pulling **something** out of a hat, and deciding to call it a rabbit


## Simulating data

Simulating data using an arbitrary number of independent tests is quite straightforward.

```{r}
# Parameter values to simulate:
N <- 200
se1 <- 0.8
sp1 <- 0.95
se2 <- 0.9
sp2 <- 0.99
se3 <- 0.95
sp3 <- 0.95

Populations <- 2
prevalence <- c(0.25,0.75)
Group <- sample(1:Populations, N, replace=TRUE)
```

---

```{r}
# Ensure replicable data:
set.seed(2020-02-18)

# Simulate the true latent state (which is unobserved in real life):
true <- rbinom(N, 1, prevalence[Group])
# Simulate test results for test 1:
test1 <- rbinom(N, 1, se1*true + (1-sp1)*(1-true))
# Simulate test results for test 2:
test2 <- rbinom(N, 1, se2*true + (1-sp2)*(1-true))
# Simulate test results for test 3:
test3 <- rbinom(N, 1, se3*true + (1-sp3)*(1-true))

simdata <- data.frame(Population=factor(Group), Test1=test1, Test2=test2, Test3=test3)
```


## Model specification

- Like for two tests, except it is now a 2x2x2 table
  
  * If calculating this manually, take **extreme** care with multinomial tabulation

- Or use autohuiwalter

  * This will also deal gracefully with missing data in one or more test results

```{r, results='hide'}
source("autohuiwalter.R")
auto_huiwalter(simdata[,c('Population','Test1','Test2','Test3')], outfile='auto3thw.bug')
```

---

```{r, echo=FALSE, comment=''}
tc <- gsub('\t','',readLines('auto3thw.bug')[c(8,9,11,19:22,54:57,59)])
cat(tc[1:3],'', tc[4:7],'', '. . . ', '', tc[8:12], sep='\n')
```


## Alternative model specification

We might want to explicitly model the latent state:

```{r include=FALSE}
glmhw_definition <- c("model{

  for(i in 1:N){
    truestatus[i] ~ dbern(prev[Population[i]])

    Status[i] ~ dcat(prob[i, ])
    prob[1:8,i] <- se_prob[1:8,i] + sp_prob[1:8,i]
", "
		se_prob[1,p] <- truestatus[i] * ((1-se[1])*(1-se[2])*(1-se[3]))
		sp_prob[1,p] <- (1-truestatus[i]) * (sp[1]*sp[2]*sp[3])

		se_prob[2,p] <- truestatus[i] * (se[1]*(1-se[2])*(1-se[3])
		sp_prob[2,p] <- (1-truestatus[i]) * ((1-sp[1])*sp[2]*sp[3])

		se_prob[3,p] <- truestatus[i] * ((1-se[1])*se[2]*(1-se[3]))
		sp_prob[3,p] <- (1-truestatus[i]) * (sp[1]*(1-sp[2])*sp[3])

		se_prob[4,p] <- truestatus[i] * (se[1]*se[2]*(1-se[3]))
		sp_prob[4,p] <- (1-truestatus[i]) * ((1-sp[1])*(1-sp[2])*sp[3])

		se_prob[5,p] <- truestatus[i] * ((1-se[1])*(1-se[2])*se[3])
		sp_prob[5,p] <- (1-truestatus[i]) * (sp[1]*sp[2]*(1-sp[3]))

		se_prob[6,p] <- truestatus[i] * (se[1]*(1-se[2])*se[3])
		sp_prob[6,p] <- (1-truestatus[i]) * ((1-sp[1])*sp[2]*(1-sp[3]))

		se_prob[7,p] <- truestatus[i] * ((1-se[1])*se[2]*se[3])
		sp_prob[7,p] <- (1-truestatus[i]) * (sp[1]*(1-sp[2])*(1-sp[3]))

		se_prob[8,p] <- truestatus[i] * (se[1]*se[2]*se[3])
		sp_prob[8,p] <- (1-truestatus[i]) * ((1-sp[1])*(1-sp[2])*(1-sp[3]))
  }
", "
	prev[1] ~ dbeta(1,1)
	prev[2] ~ dbeta(1,1)

  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)T(1-sp[2], )
  sp[2] ~ dbeta(1, 1)
  se[3] ~ dbeta(1, 1)T(1-sp[3], )
  sp[3] ~ dbeta(1, 1)

  #data# Status, N, Population
  #monitor# prev, se, sp
  #inits# prev, se, sp
}
")
cat(glmhw_definition, sep='', file='glm_hw3t.bug')
```


```{r comment='', echo=FALSE}
cat(glmhw_definition[1], sep='\n')
```

---

```{r comment='', echo=FALSE}
cat(glmhw_definition[2], sep='\n')
```

---

```{r comment='', echo=FALSE}
cat(glmhw_definition[3], sep='\n')
```

---

But this is inefficient

There is also no way to distinguish individuals within the same boxes

We could also use the estimated se/sp/prev to post-calculate these status probabilities

This is useful for post-hoc ROC


## Exercise {.fragile}

Simulate data from 3 tests and analyse using the autohuiwalter function

Do the estimates of Se/Sp correspond to the simulation parameters?

Make some data missing for one or more tests and re-generate the model

  * Can you see what has changed in the code?


`r if(params$presentation) {"\\begin{comment}"}`

## Solution {.fragile}


`r if(params$presentation) {"\\end{comment}"}`

## Optional Exercise {.fragile}

Simulate data from 3 antibody tests with ab positive step [give code in solution]

Does the se/sp estimated by a model recover the parameters?

Why not?



`r if(params$presentation) {"\\begin{comment}"}`

## Optional Solution {.fragile}


```{r}

## Parameters

# True prevalence:
prev <- 0.2

# Probability of antibody response conditional on disease status:
se_antibody <- 0.95
sp_antibody <- 0.99

# Probability of ELISA target presence conditional on antibody status:
se_target <- 0.95
sp_target <- 0.99

# Probability of ELISA test positive conditional on target presence:
se_elisa_cond <- 0.99
sp_elisa_cond <- 0.99

# Probability of Western Blot positive conditional on antibody status:
se_wb_cond <- 0.9
sp_wb_cond <- 0.99


## Derived parameters:

# Overall Se/Sp of Western Blot:
se_wb <- se_antibody*se_wb_cond + (1-se_antibody)*(1-sp_wb_cond)
sp_wb <- sp_antibody*sp_wb_cond + (1-sp_antibody)*(1-se_wb_cond)

# Overall Se/Sp of ELISA:
se_abtg <- se_antibody*se_target + (1-se_antibody)*(1-sp_target)
sp_abtg <- sp_antibody*sp_target + (1-sp_antibody)*(1-se_target)
se_elisa <- se_abtg*se_elisa_cond + (1-se_abtg)*(1-sp_target)
sp_elisa <- sp_abtg*sp_elisa_cond + (1-sp_abtg)*(1-se_target)

se_wb
sp_wb
se_elisa
sp_elisa



## Simulation

iters <- 10^6

library('tidyverse')

simdata <- data.frame(TrueStatus = rbinom(iters, 1, prev)) %>%
	mutate(AntibodyStatus = rbinom(iters, 1, TrueStatus*se_antibody + (1-TrueStatus)*(1-sp_antibody))) %>%
	mutate(TargetStatus = rbinom(iters, 1, AntibodyStatus*se_target + (1-AntibodyStatus)*(1-sp_target))) %>%
	mutate(ELISA1 = rbinom(iters, 1, TargetStatus*se_elisa_cond + (1-TargetStatus)*(1-sp_elisa_cond))) %>%
	mutate(ELISA2 = rbinom(iters, 1, TargetStatus*se_elisa_cond + (1-TargetStatus)*(1-sp_elisa_cond))) %>%
	mutate(WesternBlot = rbinom(iters, 1, AntibodyStatus*se_wb_cond + (1-AntibodyStatus)*(1-sp_wb_cond)))

str(simdata)

# Verify that the tests are correlated:
table(simdata$ELISA1, simdata$ELISA2)
table(simdata$ELISA1, simdata$WesternBlot)


## Look at results:

# The individual test-level Se and Sp are as expected:

simdata %>%
	group_by(TrueStatus) %>%
	mutate(Total = n()) %>%
	gather(Test, Result, -Total, -TrueStatus) %>%
	group_by(TrueStatus, Test, Total) %>%
	summarise(PercentPositive = sum(Result)/Total[1] *100) %>%
	filter(Test %in% c('ELISA1','ELISA2','WesternBlot'))
# Compare to:
(1-sp_elisa)*100
(1-sp_wb)*100
se_elisa*100
se_wb*100

# Serial testing with all 3 tests:
simdata %>%
	# Define positive as needing to be positive for all tests
	mutate(Combined = ELISA1 * ELISA2 * WesternBlot) %>%
	group_by(TrueStatus) %>%
	mutate(Total = n()) %>%
	gather(Test, Result, -Total, -TrueStatus) %>%
	group_by(TrueStatus, Test, Total) %>%
	summarise(PercentPositive = sum(Result)/Total[1] *100) %>%
	filter(Test %in% c('ELISA1','ELISA2','WesternBlot','Combined'))
# So serial testing has sensitivity of ~80% and specificity of ~99.1%

# You could also try other testing procedures
# But note that the overall sensitivity/specificity is sensitive to all 8 of the se/sp parameters at the top!
```

`r if(params$presentation) {"\\end{comment}"}`


# Session 3b:  Hui-Walter models for multiple tests with conditional depdendence

## Branching of processes leading to test results

Example:  two antibody tests and one antigen test

Or three antibody tests where one has a different target to the other two


## Simulating data

It helps to consider the data simulation as a biological process.  

```{r}
# Parameter values to simulate:
N <- 200
se1 <- 0.8; sp1 <- 0.95
se2 <- 0.9; sp2 <- 0.99
se3 <- 0.95; sp3 <- 0.95

Populations <- 2
prevalence <- c(0.25,0.75)
Group <- rep(1:Populations, each=N)

# Ensure replicable data:
set.seed(2017-11-21)

# The probability of an antibody response given disease:
abse <- 0.8
# The probability of no antibody response given no disease:
absp <- 1 - 0.2
```

---

```{r}
# Simulate the true latent state:
true <- rbinom(N*Populations, 1, prevalence[Group])
# Simulate test results for test 1:
test1 <- rbinom(N*Populations, 1, se1*true + (1-sp1)*(1-true))
# Tests 2 & 3 will be co-dependent on antibody response:
antibody <- rbinom(N*Populations, 1, abse*true + (1-absp)*(1-true))
# Simulate test 2 & 3 results based on this other latent state:
test2 <- rbinom(N*Populations, 1, se2*antibody + (1-sp2)*(1-antibody))
test3 <- rbinom(N*Populations, 1, se3*antibody + (1-sp3)*(1-antibody))

ind3tests <- data.frame(Population=Group, Test1=test1, Test2=test2, Test3=test3)
```

---

```{r}
# The overall sensitivity of the correlated tests is:
abse*se2 + (1-abse)*(1-sp2)
abse*se3 + (1-abse)*(1-sp3)

# The overall specificity of the correlated tests is:
absp*sp2 + (1-absp)*(1-se2)
absp*sp3 + (1-absp)*(1-se3)
```

. . .

We need to think carefully about what we are conditioning on when interpreting sensitivity and specificity!

## Model specification

```{r, eval=FALSE}

	se_prob[1,p] <- prev[p] * ((1-se[1])*(1-se[2])*(1-se[3]) +covse12 +covse13 +covse23)
	sp_prob[1,p] <- (1-prev[p]) * (sp[1]*sp[2]*sp[3] +covsp12 +covsp13 +covsp23)

	se_prob[2,p] <- prev[p] * (se[1]*(1-se[2])*(1-se[3]) -covse12 -covse13 +covse23)
	sp_prob[2,p] <- (1-prev[p]) * ((1-sp[1])*sp[2]*sp[3] -covsp12 -covsp13 +covsp23)

	...
		
	# Covariance in sensitivity between tests 1 and 2:
	covse12 ~ dunif( (se[1]-1)*(1-se[2]) , min(se[1],se[2]) - se[1]*se[2] )
	# Covariance in specificity between tests 1 and 2:
	covsp12 ~ dunif( (sp[1]-1)*(1-sp[2]) , min(sp[1],sp[2]) - sp[1]*sp[2] )
	
	...

```


## Generating the model

Use autohuiwalter with argument covon=TRUE

```{r, results='hide'}
source('autohuiwalter.R')
auto_huiwalter(ind3tests, 'auto3tihw.bug', covon=TRUE)
```

```{r, echo=FALSE, comment=''}
cat(gsub('\t','',readLines('auto3tihw.bug')[87:92]), sep='\n')
```

---

```{r, echo=FALSE, comment=''}
cat(gsub('\t','',readLines('auto3tihw.bug')[94:106]), sep='\n')
```


## Exercise {.fragile}

Simulate data with a dependence between 2 tests

Model assuming conditional independence biases the estimates

Turn on covariance between the two tests

Model with conditional depdendence has bigger CI but unbiased


`r if(params$presentation) {"\\begin{comment}"}`

## Solution {.fragile}


`r if(params$presentation) {"\\end{comment}"}`

## Optional Exercise {.fragile}

Activate covariance between all 3 tests

`r if(params$presentation) {"\\begin{comment}"}`

## Optional Solution {.fragile}


`r if(params$presentation) {"\\end{comment}"}`


# Session 3c:  Model selection

## Motivation

- Choosing between candidate models
  * DIC
  * Bayes Factors
  * BIC
  * WAIC

. . .

- Assessing model adequacy:
  * Verify using a simulation study
  * Posterior predictive p-values
  * Comparison of results from different models eg:
      * Independence vs covariance
      * Different priors

. . .

Others?  Discussion!

## DIC and WAIC

- DIC
  * Works well for hierarchical normal models
  * To calculate:
    * Add dic and ped to the monitors in runjags
    * But don't trust the results for these types of models

- WAIC
  * Approximation to LOO
  * Needs independent likelihoods
    * Could work for individual-level models?
  * Currently a pain to calculate
    * See WAIC.R in the GitHub directory
    * And/or wait for updates to runjags (and particularly JAGS 5)

. . .

```{r, eval=FALSE}
install.packages('runjags', repos=c("https://ku-awdc.github.io/drat/", "https://cran.rstudio.com/"))
```


## Discussion and free practical time

Any questions?

```{r cleanup, include=FALSE}
unlink('glm_hw3t.bug')
unlink('auto3thw.bug')
unlink('auto3tihw.bug')
```
