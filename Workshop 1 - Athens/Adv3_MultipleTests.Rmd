---
title: Hands-on training session 3
subtitle: Hui-Walter models with more than two diagnostic tests
date: "`r Sys.Date()`"
author:
  - Matt Denwood
  - Giles Innocent
  - Sonja Hartnack
theme: metropolis
aspectratio: 43
colortheme: seahorse
header-includes: 
  - \input{preamble}
params:
  presentation: TRUE
output:
  beamer_presentation:
      pandoc_args: ["-t", "beamer"]
      slide_level: 2
  html_document: default
---

```{r rendering, eval=FALSE, include=FALSE}
# To render this as PDF (beamer) slides run:
rmarkdown::render('Adv3_MultipleTests.Rmd', 'beamer_presentation', params=list(presentation=TRUE))
# And for html:
rmarkdown::render('Adv3_MultipleTests.Rmd', 'html_document', params=list(presentation=FALSE))
```

```{r setup, include=FALSE}
# Reduce the width of R code output for PDF only:
if(params$presentation) options(width=60)
knitr::opts_chunk$set(echo = TRUE)

library('runjags')
runjags.options(silent.jags=TRUE)

set.seed(2020-02-18)
```


# Introduction

## Overview

Date/time:

  - 20th February 2020
  - 14.00 - 15.30

Teachers:

  - Matt Denwood (presenter)
  - Giles Innocent
  - Sonja Hartnack


## Recap

- JAGS / runjags is the wasy way to work with complex models
  * But we *still have to* check convergence and effective sample size!

- Estimating sensitivity and specificity is like pulling a rabbit out of a hat

  * Multiple populations helps **a lot**
  * Strong priors for one of the tests helps even more!

. . .

- But what if the tests are not independent of each other?

# Session 3a:  Hui-Walter models for multiple tests with conditional indepdendence

## What exactly is our latent class?

- What do we mean by "conditionally independent?"

. . .

- Example:  we have three antibody tests
  
  * The latent status is actually 'producing antibodies' not 'diseased'
  
. . .

- Example:  antibody vs egg count tests for liver fluke

  * Does the latent state include migrating juvenile fluke?

. . .

- We're actually pulling **something** out of a hat, and deciding to call it a rabbit


## Simulating data

Simulating data using an arbitrary number of independent tests is quite straightforward.

```{r}
# Parameter values to simulate:
N <- 200
se1 <- 0.8
sp1 <- 0.95
se2 <- 0.9
sp2 <- 0.99
se3 <- 0.95
sp3 <- 0.95

Populations <- 2
prevalence <- c(0.25,0.75)
Group <- sample(1:Populations, N, replace=TRUE)
```

---

```{r}
# Ensure replicable data:
set.seed(2020-02-18)

# Simulate the true latent state (which is unobserved in real life):
true <- rbinom(N, 1, prevalence[Group])
# Simulate test results for test 1:
test1 <- rbinom(N, 1, se1*true + (1-sp1)*(1-true))
# Simulate test results for test 2:
test2 <- rbinom(N, 1, se2*true + (1-sp2)*(1-true))
# Simulate test results for test 3:
test3 <- rbinom(N, 1, se3*true + (1-sp3)*(1-true))

simdata <- data.frame(Population=factor(Group), Test1=test1, Test2=test2, Test3=test3)
```


## Model specification

- Like for two tests, except it is now a 2x2x2 table
  
  * If calculating this manually, take **extreme** care with multinomial tabulation

- Or use autohuiwalter

  * This will also deal gracefully with missing data in one or more test results

```{r, results='hide'}
source("autohuiwalter.R")
auto_huiwalter(simdata[,c('Population','Test1','Test2','Test3')], outfile='auto3thw.bug')
```

---

```{r, echo=FALSE, comment=''}
tc <- readLines('auto3thw.bug')[c(8,9,11,12,17,19:22,54:57,59)]
cat(tc[1:4],'', '\t. . . ', '', tc[5:9],'', '\t\t. . . ', '', tc[10:14], sep='\n')
```


## Alternative model specification

We might want to explicitly model the latent state:

```{r include=FALSE}
glmhw_definition <- c("model{

  for(i in 1:N){
    truestatus[i] ~ dbern(prev[Population[i]])

    Status[i] ~ dcat(prob[i, ])
    prob[1:8,i] <- se_prob[1:8,i] + sp_prob[1:8,i]
", "
    se_prob[1,p] <- truestatus[i] * ((1-se[1])*(1-se[2])*(1-se[3]))
		sp_prob[1,p] <- (1-truestatus[i]) * (sp[1]*sp[2]*sp[3])

		se_prob[2,p] <- truestatus[i] * (se[1]*(1-se[2])*(1-se[3])
		sp_prob[2,p] <- (1-truestatus[i]) * ((1-sp[1])*sp[2]*sp[3])

		se_prob[3,p] <- truestatus[i] * ((1-se[1])*se[2]*(1-se[3]))
		sp_prob[3,p] <- (1-truestatus[i]) * (sp[1]*(1-sp[2])*sp[3])

		se_prob[4,p] <- truestatus[i] * (se[1]*se[2]*(1-se[3]))
		sp_prob[4,p] <- (1-truestatus[i]) * ((1-sp[1])*(1-sp[2])*sp[3])
		se_prob[5,p] <- truestatus[i] * ((1-se[1])*(1-se[2])*se[3])
		sp_prob[5,p] <- (1-truestatus[i]) * (sp[1]*sp[2]*(1-sp[3]))

		se_prob[6,p] <- truestatus[i] * (se[1]*(1-se[2])*se[3])
		sp_prob[6,p] <- (1-truestatus[i]) * ((1-sp[1])*sp[2]*(1-sp[3]))

		se_prob[7,p] <- truestatus[i] * ((1-se[1])*se[2]*se[3])
		sp_prob[7,p] <- (1-truestatus[i]) * (sp[1]*(1-sp[2])*(1-sp[3]))

		se_prob[8,p] <- truestatus[i] * (se[1]*se[2]*se[3])
		sp_prob[8,p] <- (1-truestatus[i]) * ((1-sp[1])*(1-sp[2])*(1-sp[3]))
  }
", "
	prev[1] ~ dbeta(1,1)
	prev[2] ~ dbeta(1,1)

  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)T(1-sp[2], )
  sp[2] ~ dbeta(1, 1)
  se[3] ~ dbeta(1, 1)T(1-sp[3], )
  sp[3] ~ dbeta(1, 1)

  #data# Status, N, Population
  #monitor# prev, se, sp
  #inits# prev, se, sp
}
")
cat(glmhw_definition, sep='', file='glm_hw3t.bug')
```


```{r comment='', echo=FALSE}
cat(glmhw_definition[1], sep='\n')
```

---

```{r comment='', echo=FALSE}
cat(glmhw_definition[2], sep='\n')
```

---

But this is inefficient

There is also no way to distinguish individuals within the same boxes

We could also use the estimated se/sp/prev to post-calculate these status probabilities

This is useful for post-hoc ROC

## Exercise

Simulate data from 3 tests and analyse using the autohuiwalter function

Do the estimates of Se/Sp correspond to the simulation parameters?

Make some data missing for one or more tests and re-generate the model

  * Can you see what has changed in the code?


## Optional Exercise

Simulate data from 3 antibody tests with ab positive step [give code in solution]

Does the se/sp estimated by a model recover the parameters?

Why not?


```{r}

library('tidyverse')

## Parameters

# True prevalence:
prev <- 0.2

# Probability of antibody response conditional on disease status:
se_antibody <- 0.95
sp_antibody <- 0.99

# Probability of ELISA target presence conditional on antibody status:
se_target <- 0.95
sp_target <- 0.99

# Probability of ELISA test positive conditional on target presence:
se_elisa_cond <- 0.99
sp_elisa_cond <- 0.99

# Probability of Western Blot positive conditional on antibody status:
se_wb_cond <- 0.9
sp_wb_cond <- 0.99


## Derived parameters:

# Overall Se/Sp of Western Blot:
se_wb <- se_antibody*se_wb_cond + (1-se_antibody)*(1-sp_wb_cond)
sp_wb <- sp_antibody*sp_wb_cond + (1-sp_antibody)*(1-se_wb_cond)

# Overall Se/Sp of ELISA:
se_abtg <- se_antibody*se_target + (1-se_antibody)*(1-sp_target)
sp_abtg <- sp_antibody*sp_target + (1-sp_antibody)*(1-se_target)
se_elisa <- se_abtg*se_elisa_cond + (1-se_abtg)*(1-sp_target)
sp_elisa <- sp_abtg*sp_elisa_cond + (1-sp_abtg)*(1-se_target)

se_wb
sp_wb
se_elisa
sp_elisa



## Simulation

iters <- 10^6

simdata <- data.frame(TrueStatus = rbinom(iters, 1, prev)) %>%
	mutate(AntibodyStatus = rbinom(iters, 1, TrueStatus*se_antibody + (1-TrueStatus)*(1-sp_antibody))) %>%
	mutate(TargetStatus = rbinom(iters, 1, AntibodyStatus*se_target + (1-AntibodyStatus)*(1-sp_target))) %>%
	mutate(ELISA1 = rbinom(iters, 1, TargetStatus*se_elisa_cond + (1-TargetStatus)*(1-sp_elisa_cond))) %>%
	mutate(ELISA2 = rbinom(iters, 1, TargetStatus*se_elisa_cond + (1-TargetStatus)*(1-sp_elisa_cond))) %>%
	mutate(WesternBlot = rbinom(iters, 1, AntibodyStatus*se_wb_cond + (1-AntibodyStatus)*(1-sp_wb_cond)))

str(simdata)

# Verify that the tests are correlated:
table(simdata$ELISA1, simdata$ELISA2)
table(simdata$ELISA1, simdata$WesternBlot)


## Look at results:

# The individual test-level Se and Sp are as expected:
simdata %>%
	group_by(TrueStatus) %>%
	mutate(Total = n()) %>%
	gather(Test, Result, -Total, -TrueStatus) %>%
	group_by(TrueStatus, Test, Total) %>%
	summarise(PercentPositive = sum(Result)/Total[1] *100) %>%
	filter(Test %in% c('ELISA1','ELISA2','WesternBlot'))
# Compare to:
(1-sp_elisa)*100
(1-sp_wb)*100
se_elisa*100
se_wb*100

# Serial testing with all 3 tests:
simdata %>%
	# Define positive as needing to be positive for all tests
	mutate(Combined = ELISA1 * ELISA2 * WesternBlot) %>%
	group_by(TrueStatus) %>%
	mutate(Total = n()) %>%
	gather(Test, Result, -Total, -TrueStatus) %>%
	group_by(TrueStatus, Test, Total) %>%
	summarise(PercentPositive = sum(Result)/Total[1] *100) %>%
	filter(Test %in% c('ELISA1','ELISA2','WesternBlot','Combined'))
# So serial testing has sensitivity of ~80% and specificity of ~99.1%

# You could also try other testing procedures
# But note that the overall sensitivity/specificity is sensitive to all 8 of the se/sp parameters at the top!
```



# Session 3b:  Hui-Walter models for multiple tests with conditional depdendence

## Branching of processes leading to test results

Example:  two antibody tests and one antigen test

Or three antibody tests where one has a different target to the other two


## Simulating data

It helps to consider the data simulation as a biological process.  

```{r}
# Parameter values to simulate:
N <- 200
se1 <- 0.8
se2 <- 0.9
se3 <- 0.95
sp1 <- 0.95
sp2 <- 0.99
sp3 <- 0.95

Populations <- 2
prevalence <- c(0.25,0.75)
Group <- rep(1:Populations, each=N)

# Ensure replicable data:
set.seed(2017-11-21)

# We will assume test 1 is dependent of the others, but tests 2&3
# are not independent (e.g. they are both antibody tests)
# The probability of an antibody response given disease positive:
abse <- 0.8
# (One minus) the probability of an antibody response given disease negative:
absp <- 1 - 0.2

# Simulate the true latent state (which is unobserved in real life):
true <- rbinom(N*Populations, 1, prevalence[Group])
# Simulate test results for test 1:
test1 <- rbinom(N*Populations, 1, se1*true + (1-sp1)*(1-true))
# Tests 2&3 will be non-independent, so simulate another biological step 
# e.g. antibody response:
antibody <- rbinom(N*Populations, 1, abse*true + (1-absp)*(1-true))
# Simulate test 2&3 results based on this other latent state:
test2 <- rbinom(N*Populations, 1, se2*antibody + (1-sp2)*(1-antibody))
test3 <- rbinom(N*Populations, 1, se3*antibody + (1-sp3)*(1-antibody))

ind3tests <- data.frame(Population=Group, Test1=test1, Test2=test2, Test3=test3)

# The overall sensitivity of the correlated tests is effectively this:
abse*se2 + (1-abse)*(1-sp2)
abse*se3 + (1-abse)*(1-sp3)

# The overall specificity of the correlated tests is effectively this:
absp*sp2 + (1-absp)*(1-se2)
absp*sp3 + (1-absp)*(1-se3)

```

## Verifying simulation results

## Model specification

```{r, eval=FALSE}

		# Probability of observing ELISA1- ELISA2- WesternBlot- from a true positive::
		se_prob[1,p] <- prev[p] * ((1-se[1])*(1-se[2])*(1-se[3]) +covse12 +covse13 +covse23)
		# Probability of observing ELISA1- ELISA2- WesternBlot- from a true negative::
		sp_prob[1,p] <- (1-prev[p]) * (sp[1]*sp[2]*sp[3] +covsp12 +covsp13 +covsp23)

		# Probability of observing ELISA1+ ELISA2- WesternBlot- from a true positive::
		se_prob[2,p] <- prev[p] * (se[1]*(1-se[2])*(1-se[3]) -covse12 -covse13 +covse23)
		# Probability of observing ELISA1+ ELISA2- WesternBlot- from a true negative::
		sp_prob[2,p] <- (1-prev[p]) * ((1-sp[1])*sp[2]*sp[3] -covsp12 -covsp13 +covsp23)

		...
		
	# Covariance in sensitivity between ELISA1 and ELISA2 tests:
	covse12 ~ dunif( (se[1]-1)*(1-se[2]) , min(se[1],se[2]) - se[1]*se[2] )
	# Covariance in specificity between ELISA1 and ELISA2 tests:
	covsp12 ~ dunif( (sp[1]-1)*(1-sp[2]) , min(sp[1],sp[2]) - sp[1]*sp[2] )
	
	...

```


## Generating the model

Extreme care needed

Use autohuiwalter with argument covon=TRUE

```{r}
source('autohuiwalter.R')
auto_huiwalter(ind3tests, 'auto3tihw.bug', covon=TRUE)
```

```{r, echo=FALSE, comment=''}
cat(readLines('auto3tihw.bug'), sep='\n')
```


## Exercise

Simulate data with a dependence between 2 tests

Model assuming conditional independence biases the estimates

Model with conditional depdendence has bigger CI but unbiased


# Session 3c:  Model selection

## Motivation

[Planning for this session to be a general discussion between all instructors and students, as I am not entirely sure what to recommend in terms of model selection - except that I dislike DIC!!!]

## Background to DIC

[Some theory slides stolen from ABME course:  ABME_Model selection.pptx]

DIC works fine for hierarchical normal models but not others


## Other methods

Bayes factors work well if you can count them

WAIC works better for a wide range of models
	
	* An approximation to LOO with general applicability
	* Probably won't work for Hui-Walter though due to lack of independent data
	* Could be useful if using the GLM version (untested!)

Models tend to be sensitive to priors

Simulating data and testing that your model recovers the parameters is a good idea

## Calculating DIC

Add dic and ped to the monitors in runjags

But don't trust the results

Also bear in mind you can't parallelise

## Calculating WAIC

Currently a pain 

```{r}
## This is an example of extracting WAIC from runjags/jags objects
# Matt Denwood, 2019-11-11
# Note that this will all get much easier with the release of JAGS 5 and the next verison of runjags!!

## A function to return the WAIC
# Also returns the effective number of parameters (p_waic), elpd and lpd as described by:
# www.stat.columbia.edu/~gelman/research/unpublished/waic_stan.pdf
# Note:  	mean_lik is the log of the (exponentiated) likelihoods
#			var_log_lik is the variance of the log likelihoods
#			these need separate monitors in JAGS
get_waic <- function(mean_lik, var_log_lik){
	
	stopifnot(length(mean_lik)==length(var_log_lik))
	stopifnot(all(mean_lik > 0))
	N <- length(mean_lik)
	
	lpd <- log(mean_lik)
	elpd <- lpd - var_log_lik
	waic <- -2 * elpd
	se <- (var(waic) / N)^0.5
	
	return(list(waic=-2*sum(elpd), p_waic=sum(var_log_lik), elpd=sum(elpd), lpd=sum(lpd), se_waic=se, pointwise=cbind(waic=waic, elpd=elpd, lpd=lpd)))
}


## An example based on Andrew Gelman's 8 schools data, to match that used in the Vehtari and Gelman (2014) paper

# Data as used by Gelman:
schools <-
structure(list(school = structure(1:8, .Label = c("A", "B", "C",
"D", "E", "F", "G", "H"), class = "factor"), estimate = c(28L,
8L, -3L, 7L, -1L, 1L, 18L, 12L), sd = c(15L, 10L, 16L, 11L, 9L,
11L, 10L, 18L)), .Names = c("school", "estimate", "sd"), class = "data.frame", row.names = c(NA,
-8L))

# Model definition:
model <- "
model {
	for (j in 1:J){  						# J = the number of schools 
		y[j] ~ dnorm (theta[j], tau.y[j])	# data model: the likelihood
		theta[j] <- mu.theta + eta[j]
		tau.y[j] <- pow(sigma.y[j], -2)
		
		# These are required to monitor the variance of the log likelihood:
		log_lik[j] <- logdensity.norm(y[j], theta[j], tau.y[j])
		# And the mean of the likelihood:
		lik[j] <- exp(log_lik[j])
	}
	for (j in 1:J){
		eta[j] ~ dnorm (0, tau.theta)
	}
	tau.theta <- pow(sigma.theta, -2)
	sigma.theta ~ dhalfcauchy(prior.scale)  # The dhalfcauchy distribution is also implemented in runjags
	mu.theta ~ dnorm (0.0, 1.0E-6)			# noninformative prior on mu
	#data# J, y, sigma.y, prior.scale
	#monitor# theta, mu.theta, sigma.theta
}"


## Run the model:
library('runjags')
library('rjags')

# Calculate the data:
J <- nrow(schools)
y <- schools$estimate
sigma.y <- schools$sd
prior.scale <- 25

# Initial run for main parameter monitoring:
results <- run.jags(model, sample=10000)
# Second run for WAIC monitors:
ll <- jags.samples(as.jags(results), c('lik', 'log_lik'), type=c('mean','variance'), 10000)
# Calculate the WAIC statistic and effective parameters etc:
get_waic(as.mcmc(ll$mean$lik)[,1], as.mcmc(ll$variance$log_lik)[,1])

```

## Future Updates

Model criticism will get better in JAGS 5, and the next update of runjags

Installing development version of runjags:

```{r}
# Put on drat server and supply code here
```

WAIC is also calculable from Stan models (easily?)

## Discussion and free practical time

What would be useful to add to the autohuiwalter function?

  * Modify so it allows Se/Sp priors to be defined as matrices?
  * And correlations on/off as matrices?

```{r cleanup, include=FALSE}
unlink('auto3thw.bug')
unlink('auto3tihw.bug')
```
