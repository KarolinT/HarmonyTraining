---
title: Hands-on training session 2
subtitle: Hui-Walter models for diagnostic test evaluation
date: "`r Sys.Date()`"
author:
  - Matt Denwood
  - Giles Innocent
theme: metropolis
aspectratio: 43
colortheme: seahorse
header-includes: 
  - \input{preamble}
params:
  presentation: TRUE
output:
  beamer_presentation:
      pandoc_args: ["-t", "beamer"]
      slide_level: 2
  html_document: default
---

```{r rendering, eval=FALSE, include=FALSE}
# To render this as PDF (beamer) slides run:
rmarkdown::render('Adv2_HuiWalter.Rmd', 'beamer_presentation', params=list(presentation=TRUE))
# And for html:
rmarkdown::render('Adv2_HuiWalter.Rmd', 'html_document', params=list(presentation=FALSE))
```

```{r setup, include=FALSE}
# Reduce the width of R code output for PDF only:
if(params$presentation) options(width=60)
knitr::opts_chunk$set(echo = TRUE)

library('runjags')
runjags.options(silent.jags=TRUE)

set.seed(2020-02-18)
```

# Introduction

## Overview

Date/time:

  - 19th February 2020
  - 16.00 - 17.00

Teachers:

  - Matt Denwood (presenter)
  - Giles Innocent

## Recap

- Fitting models using MCMC is easy with JAGS / runjags

- But we must **never forget** to check convergence and effective sample size!

- More complex models become easy to implement

  * For example imperfect diagnostic tests
  * But remember to be realistic about what is possible with your data

. . .

- So how do we extend these models to multiple diagnostic tests?


# Session 2a:  Hui-Walter models for 2 tests and 1 population

## Hui-Walter Model

- A particular model formulation that was originally designed for evaluating diagnostic tests in the absence of a gold standard

- Not necessarily (or originally) Bayesian but now usually implemented using Bayesian MCMC
  
- But evaluating an imperfect test against another imperfect test is a bit like pulling a rabbit out of a hat
  * If we don't know the true disease status, how can we estimate sensitivity or specificity for either test?


## Model Specification


```{r include=FALSE}
hw_definition <- c("model{
  Tally ~ dmulti(prob, TotalTests)
  
  # Test1- Test2-
	prob[1] <- (prev * ((1-se[1])*(1-se[2]))) + ((1-prev) * ((sp[1])*(sp[2])))

  # Test1+ Test2-
	prob[2] <- (prev * ((se[1])*(1-se[2]))) + ((1-prev) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob[3] <- (prev * ((1-se[1])*(se[2]))) + ((1-prev) * ((sp[1])*(1-sp[2])))
", " 
  # Test1+ Test2+
	prob[4] <- (prev * ((se[1])*(se[2]))) + ((1-prev) * ((1-sp[1])*(1-sp[2])))

  prev ~ dbeta(1, 1)
  se[1] ~ dbeta(1, 1)
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)
  sp[2] ~ dbeta(1, 1)

  #data# Tally, TotalTests
  #monitor# prev, prob, se, sp
  #inits# prev, se, sp
}
")
cat(hw_definition, sep='', file='basic_hw.bug')
```


```{r comment='', echo=FALSE}
cat(hw_definition[1], sep='\n')
```

---

```{r comment='', echo=FALSE}
cat(hw_definition[2], sep='\n')
```

---

```{r}
twoXtwo <- matrix(c(48, 12, 4, 36), ncol=2, nrow=2)
twoXtwo
```


```{r, results='hide'}
library('runjags')

Tally <- as.numeric(twoXtwo)
TotalTests <- sum(Tally)

prev <- list(chain1=0.05, chain2=0.95)
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

results <- run.jags('basic_hw.bug', n.chains=2)
```

[Remember to check convergence and effective sample size!]

---

```{r, eval=FALSE}
results
```

```{r echo=FALSE}
res <- summary(results)[,c(1:3,9,11)]
res[] <- round(res, 3)
knitr::kable(res)
```

- Note the wide confidence intervals!


## Practicalities

- These models need A LOT of data
  * And/or strong priors for one of the tests

- Convergence is more problematic than usual

- Be **very** vareful with the order of combinations in dmultinom!

- Check your results carefully to ensure they make sense!


## Label Switching

How to interpret a test with Se=0% and Sp=0%?

. . .

  * The test is perfect - we are just holding it upside down...

. . .

We can force se+sp >= 1:

```{r eval=FALSE}
  se[1] ~ dbeta(1, 1)
  sp[1] ~ dbeta(1, 1)T(1-se[1], )
```

Or:

```{r eval=FALSE}
  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
```

But not both!

This allows the test to be useless, but not worse than useless


## Simulating data

Analysing simulated data is useful to check that we can recover parameter values.

```{r}
se1 <- 0.9; sp1 <- 0.95;
se2 <- 0.8; sp2 <- 0.99
prevalence <- 0.5; N <- 100

truestatus <- rbinom(N, 1, prevalence)
Test1 <- rbinom(N, 1, (truestatus * se1) + ((1-truestatus) * (1-sp1)))
Test2 <- rbinom(N, 1, (truestatus * se2) + ((1-truestatus) * (1-sp2)))

twoXtwo <- table(Test1, Test2)
Tally <- as.numeric(twoXtwo)
```

Can we recover these parameter values?

## Exercise {.fragile}

Modify the code in the Hui Walter model to force tests to be no worse than useless

Simulate data and recover parameters for:

  * N=10, N=100, N=1000

`r if(params$presentation) {"\\begin{comment}"}`

## Solution {.fragile}

Model definition:

```{r include=FALSE}
hw_definition <- "model{
  Tally ~ dmulti(prob, TotalTests)
  
  # Test1- Test2-
	prob[1] <- (prev * ((1-se[1])*(1-se[2]))) + ((1-prev) * ((sp[1])*(sp[2])))

  # Test1+ Test2-
	prob[2] <- (prev * ((se[1])*(1-se[2]))) + ((1-prev) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob[3] <- (prev * ((1-se[1])*(se[2]))) + ((1-prev) * ((sp[1])*(1-sp[2])))

  # Test1+ Test2+
	prob[4] <- (prev * ((se[1])*(se[2]))) + ((1-prev) * ((1-sp[1])*(1-sp[2])))
 
  prev ~ dbeta(1, 1)
  se[1] ~ dbeta(HPSe[1,1], HPSe[1,2])T(1-sp[1], )
  sp[1] ~ dbeta(HPSp[1,1], HPSp[1,2])
  se[2] ~ dbeta(HPSe[2,1], HPSe[2,2])T(1-sp[2], )
  sp[2] ~ dbeta(HPSp[2,1], HPSp[2,2])

  #data# Tally, TotalTests, HPSe, HPSp
  #monitor# prev, prob, se, sp
  #inits# prev, se, sp
}
"
cat(hw_definition, file='basic_hw.bug')
```

```{r comment='', echo=FALSE}
cat(hw_definition, sep='\n')
```

Note that we specify the prior hyperparameters as data so we can change these from R without havÃ­ng to edit the model file (this is optional!)

```{r}
se1 <- 0.9
sp1 <- 0.95
se2 <- 0.8
sp2 <- 0.99
prevalence <- 0.5

# Change N to be 10, 100 or 1000:
N <- 100

truestatus <- rbinom(N, 1, prevalence)
Test1 <- rbinom(N, 1, (truestatus * se1) + ((1-truestatus) * (1-sp1)))
Test2 <- rbinom(N, 1, (truestatus * se2) + ((1-truestatus) * (1-sp2)))

twoXtwo <- table(Test1, Test2)
twoXtwo

library('runjags')

Tally <- as.numeric(twoXtwo)
TotalTests <- sum(Tally)
HPSe <- matrix(c(1,1,1,1), nrow=2, ncol=2)
HPSp <- matrix(c(1,1,1,1), nrow=2, ncol=2)

prev <- list(chain1=0.05, chain2=0.95)
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

results <- run.jags('basic_hw.bug', n.chains=2)
results
```

How well do we recover our parameters?

```{r}
se1
se2
sp1
sp2
```

Not that well!

`r if(params$presentation) {"\\end{comment}"}`


## Optional Exercise {.fragile}

Use priors for test1 taken from session 1 and compare the results

`r if(params$presentation) {"\\begin{comment}"}`

## Optional Solution {.fragile}

```{r}
HPSe[1,] <- c(148.43, 16.49)
HPSp[1,] <- c(240.03, 12.63)

HPSe
HPSp

results <- run.jags('basic_hw.bug', n.chains=2)
results
```

How well do we recover our parameters for test 2?

```{r}
se1
se2
sp1
sp2
```

A bit better!  But note that the confidence interval for test 1 is not much narrower than that of the prior:

```{r}
# Sensitivity:
qbeta(c(0.025, 0.5, 0.975), 148.43, 16.49)
# Specificity:
qbeta(c(0.025, 0.5, 0.975), 240.03, 12.63)
```

So we have not gained any additional information about test 1.

`r if(params$presentation) {"\\end{comment}"}`

# Session 2b:  Hui-Walter models for 2 tests and N populations

## Hui-Walter models with multiple populations

- Basically an extension of the single-population model

- Works best with multiple populations each with differing prevalences
  * These could even be subgroups of individuals within the same population if there are e.g. known risk factors for disease status
  * Remember that the focus is usually to estimate the diagnostic test parameters and not the prevalence in the different populations/subgroups!


## Independent intercepts for populations

```{r eval=FALSE}
model{
  for(p in 1:Populations){
    Tally[1:4, p] ~ dmulti(prob[1:4, p], TotalTests[p])
    # Test1- Test2- Pop1
	  prob[1, p] <- (prev[p] * ((1-se[1])*(1-se[2]))) + ((1-prev[p]) * ((sp[1])*(sp[2])))
    ## etc ##
    prev[p] ~ dbeta(1, 1)
  }

  se[1] ~ dbeta(HPSe[1,1], HPSe[1,2])T(1-sp[1], )
  sp[1] ~ dbeta(HPSp[1,1], HPSp[1,2])
  se[2] ~ dbeta(HPSe[2,1], HPSe[2,2])T(1-sp[2], )
  sp[2] ~ dbeta(HPSp[2,1], HPSp[2,2])

  #data# Tally, TotalTests, Populations, HPSe, HPSp
  #monitor# prev, prob, se, sp
  #inits# prev, se, sp
}
```


## Auto Hui-Walter

We would usually start with individual-level data in a dataframe:

```{r}
se1 <- 0.9; sp1 <- 0.95; sp2 <- 0.99; se2 <- 0.8
prevalences <- c(0.1, 0.5, 0.9)
N <- 100

simdata <- data.frame(Population = sample(seq_along(prevalences), N, replace=TRUE))
simdata$probability <- prevalences[simdata$Population]
simdata$truestatus <- rbinom(N, 1, simdata$probability)
simdata$Test1 <- rbinom(N, 1, (simdata$truestatus * se1) + ((1-simdata$truestatus) * (1-sp1)))
simdata$Test2 <- rbinom(N, 1, (simdata$truestatus * se2) + ((1-simdata$truestatus) * (1-sp2)))
```

```{r, eval=FALSE}
head(simdata)
```

---

```{r, echo=FALSE}
head(simdata)
```

[Except that probability and truestatus would not normally be known!]

---

The model code and data format for an arbitrary number of populations (and tests) can be determined automatically

There is a function (provided in the GitHub repo) that can do this for us:

```{r, results='hide'}
simdata$Population <- factor(simdata$Population, levels=seq_along(prevalences), labels=paste0('Pop_', seq_along(prevalences)))

source("autohuiwalter.R")
auto_huiwalter(simdata[,c('Population','Test1','Test2')], outfile='autohw.bug')
```

---

This generates self-contained model/data/initial values etc (ignore covse and covsp for now):

```{r echo=FALSE, comment=''}
cat(readLines('autohw.bug')[-(1:2)], sep='\n')
```

---

And can be run directly from R:

```{r, results='hide'}
results <- run.jags('autohw.bug')
results
```

```{r echo=FALSE}
res <- summary(results)[,c(1:3,9,11)]
res[] <- round(res, 3)
knitr::kable(res)
```

---

- Modifying priors must still be done directly in the model file

- The model needs to be re-generated if the data changes
  * But remember that your modified priors will be reset

- There must be a single column for the population (as a factor), and all of the other columns (either factor, logical or numeric) are interpreted as being test results

- The function will soon be included in the runjags package
  * Feedback welcome!


## Observation-level model specification

```{r include=FALSE}
glmhw_definition <- c("model{

  for(i in 1:N){
    Status[i] ~ dcat(prob[i, ])
  
	  prob[i,1] <- (prev[i] * ((1-se[1])*(1-se[2]))) + 
	              ((1-prev[i]) * ((sp[1])*(sp[2])))
	  prob[i,2] <- (prev[i] * ((se[1])*(1-se[2]))) + 
	              ((1-prev[i]) * ((1-sp[1])*(sp[2])))
	  prob[i,3] <- (prev[i] * ((1-se[1])*(se[2]))) + 
	              ((1-prev[i]) * ((sp[1])*(1-sp[2])))
	  prob[i,4] <- (prev[i] * ((se[1])*(se[2]))) + 
	              ((1-prev[i]) * ((1-sp[1])*(1-sp[2])))
	  
	  logit(prev[i]) <- intercept + population_effect[Population[i]]
  }
", "
  intercept ~ dnorm(0, 0.33)
  population_effect[1] <- 0
  for(p in 2:Pops){
    population_effect[p] ~ dnorm(0, 0.1)
  }
  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)T(1-sp[2], )
  sp[2] ~ dbeta(1, 1)

  #data# Status, N, Population, Pops
  #monitor# intercept, population_effect, se, sp
  #inits# intercept, population_effect, se, sp
}
")
cat(glmhw_definition, sep='', file='glm_hw.bug')
```


```{r comment='', echo=FALSE}
cat(glmhw_definition[1], sep='\n')
```

---

```{r comment='', echo=FALSE}
cat(glmhw_definition[2], sep='\n')
```

---

- The main difference is the prior for prevalence in each population

- We also need to give initial values for `intercept` and `population_effect` rather than `prev`, and tell `run.jags` the data frame from which to extract the data (except `N` and `Pops`):

```{r, results='hide'}
intercept <- list(chain1=-1, chain2=1)
population_effect <- list(chain1=c(NA, 1, -1), chain2=c(NA, -1, 1))
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

simdata$Status <- with(simdata, factor(interaction(Test1, Test2), levels=c('0.0','1.0','0.1','1.1')))
N <- nrow(simdata)
Pops <- length(levels(simdata$Population))
glm_results <- run.jags('glm_hw.bug', n.chains=2, data=simdata)
```

---

Also like in session 1, the estimates for se/sp should be similar, although this model runs more slowly.

```{r echo=FALSE}
if(!params$presentation){
  cat('Results from the HW model:\n\n\n')
  results
}
```

```{r echo=FALSE}
if(!params$presentation){
  cat('Results from the GLM model:\n\n\n')
  glm_results
}
```

Note:  this model could be used as the basis for adding covariates

For a handy way to generate a GLM model see runjags::template.jags

  * Look out for integration with autohuiwalter in the near (ish) future...


## Exercise

Play around with the autohuiwalter function

Notice the model and data and initial values are in a self contained file

Ignore the covse and covsp for now

[There is no particular solution to this exercise!]


## Summary

- Estimating sensitivity and specificity is like pulling a rabbit out of a hat

- Multiple populations helps **a lot**

- Strong priors for one of the tests helps even more!

- Make sure you tabulate the data correctly ... or use the automated model generator!


```{r cleanup, include=FALSE}
unlink('autohw.bug')
unlink('basic_hw.bug')
unlink('glm_hw.bug')
```
